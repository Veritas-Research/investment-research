**Date:** January 24, 2026  
**Author:** Mark A

---

## Executive Summary (2-Minute Version)

**The Thesis:** Everyone's debating which AI architecture wins, scaled LLMs, world models, symbolic AI, or hybrid systems. The real question is: what do _all_ of them need? The answer is high-bandwidth memory (HBM). I'm betting on the picks-and-shovels of the AI revolution, not the models themselves.

**The Play:** 60% SK hynix / 40% Micron (5% of total portfolio)

**Expected Returns:** +16.3% CAGR blended over 5 years

- SK hynix: +19.0% CAGR (market leader, cheaper valuation)
    
- Micron: +12.9% CAGR (CHIPS Act subsidy, political protection)
    

**Why This Works:**

- HBM supply is sold out through 2027
    
- Only 3 global suppliers (SK hynix 62%, Micron 21%, Samsung 17%)
    
- Barriers to entry are massive (10+ year development, $15-20B per fab)
    
- Every AI paradigm (neural, symbolic, hybrid, world models) needs memory
    
- Test-time compute scaling multiplies inference memory demand
    

**Key Risks:**

1. Tariffs on Korean chips (10/60/20/10% probability distribution)
    
2. AI winter / demand collapse
    
3. Energy constraints limit deployment
    
4. Memory cycle peaks earlier than expected (2027 vs 2028)
    

**Next Milestone:** April 14, 2026 USTR report clarifies semiconductor tariff policy

---

**Navigation:**

- [Skip to The Thesis →](https://www.perplexity.ai/search/pull-the-transcript-from-this-HKx78ZRGS2GUt5AWa0NsQw#part-3-the-thesis)
    
- [Skip to Stock Analysis →](https://www.perplexity.ai/search/pull-the-transcript-from-this-HKx78ZRGS2GUt5AWa0NsQw#part-4-stock-analysis-sk-hynix-vs-micron)
    
- [Skip to Allocation & Math →](https://www.perplexity.ai/search/pull-the-transcript-from-this-HKx78ZRGS2GUt5AWa0NsQw#part-5-allocation--math)
    
- [Skip to Risks →](https://www.perplexity.ai/search/pull-the-transcript-from-this-HKx78ZRGS2GUt5AWa0NsQw#part-6-risks-and-monitoring)
    

---

## Table of Contents

## Part 1: The Journey (Skippable)

- [How I Got Here](https://www.perplexity.ai/search/pull-the-transcript-from-this-HKx78ZRGS2GUt5AWa0NsQw#how-i-got-here)
    
- [The Contrarian Question](https://www.perplexity.ai/search/pull-the-transcript-from-this-HKx78ZRGS2GUt5AWa0NsQw#the-contrarian-question)
    

## Part 2: Technical Deep Dive (Skippable)

- [Gary Marcus: The Skeptic Worth Listening To](https://www.perplexity.ai/search/pull-the-transcript-from-this-HKx78ZRGS2GUt5AWa0NsQw#gary-marcus-the-skeptic-worth-listening-to)
    
- [Inference vs Training: The Shift](https://www.perplexity.ai/search/pull-the-transcript-from-this-HKx78ZRGS2GUt5AWa0NsQw#inference-vs-training-the-shift)
    
- [Test-Time Compute Scaling](https://www.perplexity.ai/search/pull-the-transcript-from-this-HKx78ZRGS2GUt5AWa0NsQw#test-time-compute-scaling)
    
- [World Models: Yann LeCun's Bet](https://www.perplexity.ai/search/pull-the-transcript-from-this-HKx78ZRGS2GUt5AWa0NsQw#world-models-yann-lecuns-bet)
    
- [Symbolic AI and Neuro-Symbolic Hybrids](https://www.perplexity.ai/search/pull-the-transcript-from-this-HKx78ZRGS2GUt5AWa0NsQw#symbolic-ai-and-neuro-symbolic-hybrids)
    
- [Why Memory Wins All Scenarios](https://www.perplexity.ai/search/pull-the-transcript-from-this-HKx78ZRGS2GUt5AWa0NsQw#why-memory-wins-all-scenarios)
    

## Part 3: The Thesis 

- [The Contrarian Insight](https://www.perplexity.ai/search/pull-the-transcript-from-this-HKx78ZRGS2GUt5AWa0NsQw#part-3-the-thesis)
    
- [Memory Across All Futures](https://www.perplexity.ai/search/pull-the-transcript-from-this-HKx78ZRGS2GUt5AWa0NsQw#memory-across-all-futures)
    
- [The NPU Uncertainty Problem](https://www.perplexity.ai/search/pull-the-transcript-from-this-HKx78ZRGS2GUt5AWa0NsQw#the-npu-uncertainty-problem)
    
- [Supply Constraint Reality](https://www.perplexity.ai/search/pull-the-transcript-from-this-HKx78ZRGS2GUt5AWa0NsQw#supply-constraint-reality)
    

## Part 4: Stock Analysis 

- [The Graham/Buffett Framework](https://www.perplexity.ai/search/pull-the-transcript-from-this-HKx78ZRGS2GUt5AWa0NsQw#the-grahambuffett-framework)
    
- [SK hynix Deep Dive](https://www.perplexity.ai/search/pull-the-transcript-from-this-HKx78ZRGS2GUt5AWa0NsQw#sk-hynix-deep-dive)
    
- [Micron Deep Dive](https://www.perplexity.ai/search/pull-the-transcript-from-this-HKx78ZRGS2GUt5AWa0NsQw#micron-deep-dive)
    
- [Why Not Samsung?](https://www.perplexity.ai/search/pull-the-transcript-from-this-HKx78ZRGS2GUt5AWa0NsQw#why-not-samsung)
    
- [Comparison Table](https://www.perplexity.ai/search/pull-the-transcript-from-this-HKx78ZRGS2GUt5AWa0NsQw#comparison-table)
    

## Part 5: Allocation & Math 

- [Tariff Probability Analysis](https://www.perplexity.ai/search/pull-the-transcript-from-this-HKx78ZRGS2GUt5AWa0NsQw#tariff-probability-analysis)
    
- [Expected Returns Methodology](https://www.perplexity.ai/search/pull-the-transcript-from-this-HKx78ZRGS2GUt5AWa0NsQw#expected-returns-methodology)
    
- [Kelly Criterion & Position Sizing](https://www.perplexity.ai/search/pull-the-transcript-from-this-HKx78ZRGS2GUt5AWa0NsQw#kelly-criterion--position-sizing)
    

## Part 6: Risks and Monitoring 

- [What Could Prove Me Wrong](https://www.perplexity.ai/search/pull-the-transcript-from-this-HKx78ZRGS2GUt5AWa0NsQw#what-could-prove-me-wrong)
    
- [Rebalancing Plan](https://www.perplexity.ai/search/pull-the-transcript-from-this-HKx78ZRGS2GUt5AWa0NsQw#rebalancing-plan)
    
- [Conviction Level](https://www.perplexity.ai/search/pull-the-transcript-from-this-HKx78ZRGS2GUt5AWa0NsQw#conviction-level)
    

---

## Part 1: The Journey

## How I Got Here

Using AI every day has fundamentally changed how I work and think. Not in the shallow "ChatGPT writes my emails" way that dominates headlines, but in the deeper sense that I can't imagine going back. It's like someone handed me a superpower and now I'm watching everyone else debate whether superpowers are real.

I use AI tools for research, analysis, writing, coding, and strategic thinking. They've made me faster, more thorough, and frankly, a better version of myself. When something becomes that embedded in your workflow, you start paying attention to the infrastructure behind it. That's where the investment thesis begins.

But here's what made me pause: everyone in the investing world is having the same conversation. Which AI model wins? Which chip company captures the value? Is NVIDIA overvalued at 50x earnings? These are valid questions, but they felt like asking which railroad company would dominate in 1870 when the real play was buying steel.

## The Contrarian Question

I started asking myself: what would Michael Burry or Steve Eisman do if they looked at AI infrastructure today? What are we missing?

Burry made his fortune betting against the obvious (subprime mortgage securities that everyone assumed were safe). Eisman saw the same trade because he asked second-order questions: not "are housing prices going up?" but "what happens when they stop?" The edge wasn't in better data, it was in asking better questions.

So I started asking: **What's the component that every AI architecture needs, regardless of which paradigm wins?**

That question sent me down a rabbit hole that started, oddly enough, with a skeptic.

In January 2026, I watched Steve Eisman's podcast featuring Gary Marcus, a neuroscientist and AI researcher who's been arguing for years that large language models have fundamental limitations. Most people dismiss Marcus as a perennial bear who's been wrong about AI's progress. I took him seriously, not because I think AI is failing, but because his critique made me realize: **the next phase of AI might look completely different from the current paradigm**.

If Marcus is right that pure LLM scaling is hitting diminishing returns, what comes next? And more importantly for investors: what infrastructure does the _next_ paradigm need?

That's when things got interesting.

---

## Part 2: Technical Deep Dive

_[Note: This section is comprehensive but skippable. If you want to jump straight to the investment thesis, go to [Part 3](https://www.perplexity.ai/search/pull-the-transcript-from-this-HKx78ZRGS2GUt5AWa0NsQw#part-3-the-thesis).]_

## Gary Marcus: The Skeptic Worth Listening To

Gary Marcus isn't some random critic tweeting hot takes. He's a cognitive scientist who did his dissertation at MIT on neural networks in the 1990s, studied how children learn language, and wrote a book in 2001 called _The Algebraic Mind_ where he predicted the hallucination problem we're seeing today, before LLMs even existed.

When GPT-3 launched and everyone declared victory for deep learning, Marcus wrote an essay called "Deep Learning Is Hitting a Wall" (2022) arguing that scaling alone wouldn't get us to AGI. OpenAI literally had an internal emoji for him because he was such a thorn in their side.

Here's why I paid attention: Marcus isn't saying AI doesn't work. He's saying **current AI (LLMs) is fundamentally limited to pattern matching and can't do genuine reasoning**. His framework breaks it down using Daniel Kahneman's _Thinking Fast and Slow_:

**System 1 (Fast Thinking):** Automatic, statistical, reflexive. This is what neural networks excel at. Pattern recognition, autocomplete, association.

**System 2 (Slow Thinking):** Deliberative, logical, reasoning-based. This is what humans do when we think carefully about cause and effect, build mental models, and apply abstract rules.

Marcus's argument: **LLMs are pure System 1. They're autocomplete on steroids.** They work surprisingly well because they've memorized the entire internet, so for most questions, they can statistically reconstruct an answer that looks right. But they don't _understand_ anything, which is why they hallucinate, fail at novel reasoning, and can't explain their logic.

## The Hallucination Problem

Marcus predicted this in 2001 before LLMs existed. Here's why it happens:

LLMs break information into tiny fragments (tokens), learn statistical relationships between fragments, and reconstruct answers by glomming fragments together. Sometimes those reconstructions are wrong, but the system has no way to know. It just presents everything with perfect confidence.

Example: ChatGPT told someone that British voiceover actor Harry Shearer was born in London. Sounds plausible, lots of British voiceover actors exist. Except Harry Shearer is American, born in Los Angeles, and was a child star on _The Jack Benny Show_. The LLM created a statistically probable but factually wrong answer by blurring together "voiceover actor" + "comedian" + "British" because those things correlate in its training data.

This isn't a bug that gets fixed with more scale. It's architectural.

## Why This Matters for Investors

If Marcus is right, and I think he's directionally correct, then the AI industry will need to evolve beyond pure LLMs. That doesn't mean AI fails. It means we're entering a new phase where different architectures emerge:

- **Test-time compute scaling** (reasoning at inference time, not just training time)
    
- **World models** (systems that understand physics and causality, not just language)
    
- **Symbolic AI** (explicit rules and logic, not just pattern matching)
    
- **Neuro-symbolic hybrids** (combining neural networks with symbolic reasoning)
    

Each of these approaches solves different problems. And here's the kicker: **every single one of them needs memory**. Some need even _more_ memory than current LLMs.

That's the insight that changed how I think about AI investing.

---

## Inference vs Training: The Shift

Let me clarify a concept that's critical to understanding the thesis: **training vs inference**.

**Training** is the "learning" phase. You take a neural network and expose it to massive amounts of data (say, the entire internet). The network adjusts its internal weights over weeks or months of computation. This happens once (or periodically when updating the model). Training is compute-intensive and expensive, GPT-4 reportedly cost $100 million to train.

**Inference** is the "using" phase. Once the model is trained, its weights are frozen. When you type a question into ChatGPT, the model generates an answer using those frozen weights. This happens billions of times per day, every time someone uses the system.

Here's the key distinction:

- **Training scales with model size.** Bigger models need more GPUs, more power, more time.
    
- **Inference scales with usage.** More users = more inference operations, even if the model stays the same size.
    

For the last two years, everyone focused on training. NVIDIA's H100 GPUs sold out because hyperscalers were building massive training clusters. But now, the frontier models (GPT-4, Gemini, Claude) are already trained. The next question is: can we get better results _without_ making models bigger?

Enter test-time compute scaling.

---

## Test-Time Compute Scaling

Test-time compute (also called inference-time compute) is the idea that instead of making the model bigger during training, you use more compute _per query_ to get better answers.

**How it works:**

Traditional approach:

- Ask GPT-4 a question
    
- Get one answer
    
- That's it
    

Test-time compute approach:

- Ask the model to generate 100 different candidate answers
    
- Use a "verifier" model to score each one
    
- Select the best answer
    
- Or use chain-of-thought reasoning to iteratively refine
    

This uses 100x more compute per query, but the answer is significantly better. OpenAI's o1 and o3 "reasoning models" work this way, they "think longer" on hard problems, using more inference compute to improve accuracy.

## Why This Matters

If test-time compute becomes the standard way to improve AI, then:

1. **Training demand plateaus** (you're not making models bigger)
    
2. **Inference demand explodes** (you're using 10-100x more compute per query)
    
3. **Memory becomes the bottleneck** (not compute)
    

Why memory? Because generating 100 candidate answers means holding all that intermediate state in memory, shuttling data back and forth between the model and the verifier. The memory bandwidth requirement multiplies.

Researchers have verified this: test-time compute can be **more effective than scaling model size**, but it creates a 10-100x multiplier on inference costs and memory requirements.

---

## World Models: Yann LeCun's Bet

Yann LeCun is one of the "godfathers of deep learning," a Turing Award winner, and until recently, Meta's Chief AI Scientist. In January 2026, he left Meta after 12 years to start AMI Labs (Advanced Machine Intelligence), raising €500 million at a €3 billion valuation.

His thesis: **LLMs are too limiting. The future of AI is world models.**

**What are world models?**

World models are AI systems that build internal simulations of how the physical world works. Instead of just predicting the next word in a sequence, they understand causality, physics, spatial relationships, and temporal dynamics.

Think of it like this:

- **LLMs predict:** "What word typically comes next?"
    
- **World models predict:** "If I push this object, physics says it will fall."
    

LeCun's vision is that AI agents need to understand the world well enough to plan sequences of actions and predict outcomes. This is critical for robotics, autonomous systems, industrial processes, and any application where the AI needs to interact with physical reality, not just generate text.

## Why This Matters for Memory

World models process video, 3D spatial data, and multi-modal sensory inputs. They simulate forward in time, holding entire world states in memory. Researchers recommend **128GB+ RAM just for 3D model generation**. Real-time world model inference for robotics or autonomous vehicles needs even more.

If world models become the dominant paradigm (and LeCun is betting €500M that they will), the memory requirements are _higher_ than current LLMs, not lower.

---

## Symbolic AI and Neuro-Symbolic Hybrids

Symbolic AI is the original approach to AI from the 1950s-1980s. It uses explicit rules, logic, and knowledge representations (like IF-THEN statements, knowledge graphs, formal logic).

**Neural networks:** Learn patterns from data, great at handling messy real-world inputs, but black-box and prone to errors.

**Symbolic AI:** Uses explicit logic, explainable and provably correct, but brittle and requires manual programming.

**Neuro-symbolic hybrids** combine both:

- Neural networks handle perception (vision, speech, pattern recognition)
    
- Symbolic systems handle reasoning (applying rules, logical consistency)
    

Here's what caught my attention: in January 2026, **Yann LeCun, a longtime critic of symbolic AI, made a major shift toward hybrid neural/symbolic systems**. Gary Marcus called this "breaking news vindicating neurosymbolic AI."

When a deep learning pioneer starts embracing symbolic approaches, it's a signal. The field recognizes that pure neural scaling isn't enough.

## Why Neuro-Symbolic Needs More Memory

Symbolic reasoning requires:

- **Knowledge graphs:** Large databases of facts and relationships
    
- **Rule databases:** Thousands or millions of IF-THEN rules
    
- **Constant memory lookups:** Traversing graphs, checking conditions, verifying logic
    
- **Irregular memory access patterns:** Not the dense matrix multiplication GPUs are optimized for
    

This is a **memory-bound workload**, not a compute-bound one. Researchers at KU Leuven are specifically studying "Efficient Hardware Architectures for Neuro-Symbolic AI" because current GPUs are severely inefficient for these operations, achieving less than 10% utilization on graph traversal and logic tasks.

If neuro-symbolic becomes standard (and the smartest people in AI are moving this direction), memory demand doesn't just grow, it explodes.

---

## Why Memory Wins All Scenarios

Here's the logical matrix I built after mapping out all the potential AI futures:

|**AI Paradigm**|**Compute Need**|**Memory Need**|**Memory Trend**|
|---|---|---|---|
|**Scaled LLMs (status quo)**|High (training)|High (large models)|Growing (bigger models)|
|**Test-time compute**|Very High (inference)|**Very High** (hold 100x states)|**Multiplies**|
|**World models**|High (simulation)|**Very High** (video/3D/state)|**Much higher than LLMs**|
|**Symbolic AI**|Low (logic ops)|**Very High** (knowledge graphs)|**Memory-bound**|
|**Neuro-symbolic hybrid**|Medium (both)|**Extremely High** (neural + symbolic)|**Highest of all**|
|**Edge inference**|Medium (distributed)|**High** (on every device)|**Distributed everywhere**|

**The pattern is clear:** No matter which paradigm wins, memory demand stays flat at worst, and more likely grows significantly.

Even in the "efficiency gains" scenario where models get smaller through compression and quantization, usage growth outpaces efficiency (Jevons paradox). More users, more edge devices, more distributed inference = more aggregate memory demand.

**Memory is the one infrastructure component that wins across all futures.**

---

## Part 3: The Thesis

## The Contrarian Insight

Everyone's focused on the GPU question: Is NVIDIA overvalued? Will AMD catch up? What about custom AI chips from Google and Amazon?

These are valid questions, but they're asking about a **paradigm-specific bottleneck**. GPUs matter for training large neural networks. If the paradigm shifts away from pure scaling (which Marcus, LeCun, and increasingly Ilya Sutskever all suggest), then GPU demand could plateau or shift to different architectures.

The contrarian question is: **What's the component that every paradigm needs, regardless of which one wins?**

The answer kept coming back to memory. High-bandwidth memory specifically.

## Memory Across All Futures

Let me walk through why memory wins in each scenario:

## Scenario 1: LLM Scaling Continues (Status Quo)

Even if pure scaling continues:

- Models get bigger → more parameters → more memory per GPU
    
- Inference volume grows → more queries per second → more memory bandwidth
    
- NVIDIA's H200 has 141GB HBM3E (vs H100's 80GB) because memory is the bottleneck
    

**Memory demand: High and growing**

## Scenario 2: Test-Time Compute Becomes Standard

If Ilya Sutskever and OpenAI are right that test-time compute is the new frontier:

- Each query uses 10-100x more compute to generate/verify multiple answers
    
- Must hold 100+ candidate solutions in memory simultaneously
    
- Memory bandwidth becomes the critical constraint (not GPU cores)
    

**Memory demand: Multiplies 10-100x per query**

## Scenario 3: World Models Dominate

If Yann LeCun's AMI Labs proves that world models are superior:

- Processing video/3D data requires 128GB+ RAM per system
    
- Real-time simulation needs to hold entire world states in memory
    
- Temporal coherence means storing sequences, not just single frames
    

**Memory demand: Much higher than current LLMs**

## Scenario 4: Neuro-Symbolic Hybrids Win

If neuro-symbolic becomes the standard (Marcus's preferred outcome):

- Neural layer needs memory for model weights (same as today)
    
- Symbolic layer needs memory for knowledge graphs and rule databases
    
- Constant lookups make this a **memory-bound workload**
    

**Memory demand: Higher than pure neural, because you need both**

## Scenario 5: Edge Inference Proliferates

If inference moves to devices (phones, cars, IoT) for latency/privacy:

- Every device needs local memory for model weights
    
- Edge deployment means distributed memory everywhere
    
- Aggregate memory demand grows even if data center demand plateaus
    

**Memory demand: Distributed, but total volume increases**

## The Key Insight

**Memory is the ONLY infrastructure component that wins across all scenarios.**

GPUs might get displaced by NPUs or custom accelerators. Data centers might shift to edge. Models might get more efficient. But memory? Memory demand either stays high or grows in every future I can envision.

That's the bet.

---

## The NPU Uncertainty Problem

One reason I'm avoiding a direct GPU play: **NPU (Neural Processing Unit) uncertainty**.

If AI shifts from training-heavy (GPUs) to inference-heavy (NPUs), then we need to ask: which NPU architecture wins?

**The problem:**

- **Google TPUs:** Proprietary, locked to Google Cloud, optimized for TensorFlow
    
- **Apple Neural Engine:** Optimized for mobile, locked to Apple devices
    
- **Qualcomm Hexagon:** Mobile edge inference, competing with Apple
    
- **Intel Gaudi, AMD MI300:** Competing with NVIDIA in data centers
    
- **Custom chips:** Meta, Amazon, Microsoft all building their own
    

There's no clear standard. Different models might run better on different hardware. The ecosystem is fragmenting.

**In contrast, HBM is architecture-agnostic.** Every AI accelerator, GPU, TPU, NPU, custom chip, needs high-bandwidth memory. Whether NVIDIA wins, Google wins, or someone else wins, they all buy HBM from the same three suppliers.

**That's why memory is the safer infrastructure play.** You're not betting on which processor architecture wins. You're betting on the component that all of them need.

---

## Supply Constraint Reality

Here's the supply/demand reality that makes this thesis urgent:

**Global HBM Supply:**

- SK hynix: **62-70% market share**
    
- Micron: **21% market share**
    
- Samsung: **17% market share**
    

**That's it. Three companies control >95% of global HBM supply.**

**Barriers to entry:**

1. **Technical complexity:** Through-Silicon Via (TSV) technology, 12-16 layer stacking, hybrid bonding at nanometer precision. Samsung failed NVIDIA qualification in 2024 despite being a semiconductor giant.
    
2. **Capital requirements:** $10-20 billion per fab, plus $2-5 billion for advanced packaging facilities.
    
3. **Time to market:** 3-5 year development cycles. Even if someone started today, they're not producing until 2028-2029.
    
4. **Ecosystem lock-in:** SK hynix has worked with NVIDIA for 8+ years. Custom HBM4E designs require deep co-development. Switching costs are enormous.
    
5. **Packaging bottlenecks:** Multi-layer bonders have 12-month backlogs. You can't just buy capacity.
    

**Supply situation:**

- **Sold out through 2027:** All three suppliers are capacity-constrained
    
- **Demand exceeds supply:** OpenAI's Stargate project alone needs 900,000 HBM wafers/month; global capacity is 350,000/month
    
- **Pricing power:** DRAM prices up 60% in 2025, HBM even higher
    

**Market growth:**

- Narrow HBM market: $7.27B (2025) → $9.18B (2026) → $54.6B (2028-2030)
    
- Broad AI memory TAM: $35B (2025) → $54.6B (2026) → $100B (2028)
    

This is a classic supply-constrained market where demand massively exceeds supply for years. The only question is which supplier captures the value.

---

## Part 4: Stock Analysis (SK hynix vs Micron)

## The Graham/Buffett Framework

When analyzing these companies, I went back to first principles from Benjamin Graham and Warren Buffett:

**Graham's principles:**

1. **Don't overpay:** Price matters. A great company at the wrong price is a bad investment.
    
2. **Margin of safety:** Buy when there's a gap between intrinsic value and market price.
    
3. **Quality matters:** Focus on businesses with durable competitive advantages.
    

**Buffett's addition:**  
"It's far better to buy a wonderful company at a fair price than a fair company at a wonderful price."

**Applied to memory:**

- **Don't overpay:** Compare forward P/E ratios, not just trailing. Memory is cyclical, need to value at cycle midpoint.
    
- **Quality:** Market leadership, technical superiority, customer relationships = moat.
    
- **Fair price:** SK hynix at 7.5-8x forward P/E vs Micron at 12.3x, is the gap justified?
    

---

## SK hynix Deep Dive

**Ticker:** 000660.KS (Korea Stock Exchange)  
**Price:** ₩765,000 (~$618 USD) as of January 24, 2026  
**Market Cap:** ~$380-395 billion

## The Business

SK hynix is a pure-play memory company. When HBM wins, SK hynix wins directly, there's no dilution from other business units.

**Market Position:**

- **62-70% HBM market share** (and UBS predicts 70% for HBM4 in 2026)
    
- **90% of NVIDIA's HBM supply** comes from SK hynix
    
- First to market with HBM4 (samples shipped March-June 2025, mass production Q3 2025)
    
- Technical leader: Samsung couldn't even pass NVIDIA's HBM3E qualification, but SK hynix did
    

**Financial Snapshot:**

- Revenue (TTM): ~$50 billion (84 trillion KRW)
    
- 2026 operating profit forecast: 55-81 trillion won ($41-60 billion)
    
- This would be **5.0-5.5x higher than the 2018 memory cycle peak**
    

**Valuation:**

- Trailing P/E: ~15x
    
- **Forward P/E: 7.5-8.04x** ← This is the key metric
    
- 10-year average P/E: 10.1x
    

**Why forward P/E matters:** Memory is cyclical. You need to value it based on where earnings are going, not where they've been. At 7.5-8x forward earnings with operating profit expected to grow 5x from the previous cycle peak, this is cheap.

## The Moats

1. **Technical leadership:** First to HBM4, best yields, NVIDIA's choice for 90% of supply
    
2. **Relationship lock-in:** 8+ years working with NVIDIA, custom designs require deep co-development
    
3. **Scale advantages:** 62-70% market share gives pricing power and R&D leverage
    
4. **Barriers to entry:** See above, nobody can replicate this in <5 years
    

## The Bull Case

If the memory super-cycle plays out (HBM market $35B → $100B by 2028):

- SK hynix captures 60-70% of that growth as market leader
    
- Operating margins expand as supply stays tight
    
- P/E multiple re-rates from 8x to 15-20x (still cheap vs tech stocks at 25-30x)
    
- Stock could 2-3x over 3-5 years
    

Analyst targets: Morgan Stanley at ₩840,000 (+9.8% from current)

## The Bear Case

**Tariff risk:** This is my biggest concern. In January 2026:

- Jan 14: Trump imposed 25% tariff on AI chips (H200, MI325X)
    
- Jan 17: Commerce Secretary Lutnick threatened 100% tariffs on memory chips: "They can pay 100% tariff, or they can build in America"
    
- This directly targets South Korean chipmakers
    

If 100% tariffs are imposed, SK hynix either:

- Pays the tariff (destroys margins)
    
- Builds a $15B+ U.S. fab (huge capex, years to complete)
    

**Currency risk:** Korean Won volatility vs USD  
**Geopolitical risk:** North Korea tensions, regional instability  
**Cycle risk:** Memory cycle peaks earlier than expected (2027 vs 2028)

## My Take

SK hynix is the better business at a cheaper price. Forward P/E of 7.5-8x for the market leader with 62% share and 90% of NVIDIA's supply is a gift. The market is discounting it for being Korean (geopolitical risk) and cyclical (fear of boom-bust).

I think those fears are overblown. The tariff threat is likely a negotiating tactic to force Korean companies to invest in U.S. fabs (which they'll do). The cycle risk is real, but supply won't catch up to demand until 2027-2028 at earliest.

**This is the value play.**

---

## Micron Deep Dive

**Ticker:** MU (NASDAQ)  
**Price:** $399 as of January 24, 2026  
**Market Cap:** ~$440 billion

## The Business

Micron is a U.S.-based memory company with diversified products (DRAM, NAND, HBM). Memory is the core business, but they're not as pure-play as SK hynix.

**Market Position:**

- **21% HBM market share** (overtook Samsung for #2 in 2025)
    
- Secondary NVIDIA supplier (diversification for NVIDIA's supply chain)
    
- Paired 36GB HBM3E with AMD's MI350 GPU
    
- Strong relationships with U.S. hyperscalers (Amazon, Microsoft, Google)
    

**Financial Snapshot:**

- Revenue: $37.4 billion (up 48.9% YoY)
    
- Q1 revenue up 56.7% YoY (accelerating growth)
    
- Net Income: $8.54 billion (up 637% YoY)
    
- Net Margin: 22.8%
    
- ROE: 15.76% (above cost of capital, creating shareholder value)
    

**Valuation:**

- Trailing P/E: 34x (expensive on backward-looking basis)
    
- **Forward P/E: 12.3x** ← This is the key metric
    
- 10-year average P/E: 19.4x
    

**Why forward P/E matters:** Micron trades at 34x trailing earnings because earnings were depressed in the trough of the memory cycle. As earnings explode (up 637% YoY), the forward P/E compresses to 12.3x. That's fair value, not expensive.

## The Moats

1. **CHIPS Act subsidy:** $6.17 billion in grants + $7.5 billion in low-interest loans (total ~$13-14B effective value)
    
2. **Political protection:** U.S. government wants domestic memory production; Micron is the only American HBM supplier
    
3. **Strategic supplier status:** U.S. companies prefer domestic supply chain for critical infrastructure
    
4. **Scale advantages:** #2 globally, large enough to compete but small enough to grow
    

## The Bull Case

If the memory super-cycle plays out:

- Micron grows from 21% to 25-30% market share (taking from Samsung)
    
- CHIPS Act subsidy gives competitive advantage: $6B+ capex covered by government
    
- U.S. customers prefer domestic supplier → pricing power
    
- Forward P/E stays at 12-15x while earnings double → stock up 50-100%
    

Analyst targets: $430-450 (+9-13% from current)

## The Bear Case

**Valuation:** At 12.3x forward P/E, there's less margin of safety than SK hynix at 8x. If the cycle disappoints, multiple compresses faster.

**Competition:** Samsung is catching up and undercutting on price (30% discounts on HBM3E). If Samsung passes NVIDIA qualification, Micron loses share.

**Execution risk:** Must successfully deploy CHIPS Act funding and ramp U.S. production. Construction delays or yield issues hurt.

## My Take

Micron is the **political hedge**. The CHIPS Act subsidy is real money ($6-14B) that gives Micron a structural cost advantage in U.S. production. If tariffs are imposed on Korean chips, Micron becomes massively advantaged overnight.

At 12.3x forward P/E, it's not cheap, but it's fair value. You're paying for:

- Protection from tariff risk
    
- Government subsidy (worth $6B+)
    
- U.S. listing convenience (no currency risk)
    
- Domestic supplier preference
    

**This is the insurance policy against the tariff scenario.**

---

## Why Not Samsung?

Samsung trades at a similar discount to SK hynix (Korean discount applies to both), but I'm avoiding it for one reason: **conglomerate dilution**.

Samsung Electronics is a massive conglomerate. Semiconductors are one division alongside:

- Smartphones (Galaxy)
    
- TVs and displays
    
- Appliances (refrigerators, washers)
    
- Consumer electronics
    

When you buy Samsung, you're getting exposure to all of that. HBM revenue gets diluted across the broader company. If HBM explodes but smartphone sales struggle, the stock underperforms.

SK hynix and Micron are pure-play memory companies. When HBM wins, they win directly.

**Plus, Samsung is behind:** Failed NVIDIA HBM3E qualification in 2024, running 3 months behind on HBM4, and currently #3 in market share (17%).

For a pure-play HBM investment, Samsung doesn't make the cut.

---

## Comparison Table

|**Metric**|**SK hynix**|**Micron**|
|---|---|---|
|**Current Price**|₩765,000 (~$618)|$399|
|**Market Cap**|$380-395B|$440B|
|**HBM Market Share**|62-70%|21%|
|**Trailing P/E**|~15x|34x|
|**Forward P/E**|**7.5-8.04x**|**12.3x**|
|**P/E Gap**|35-40% cheaper|Base case|
|**Revenue (TTM)**|$50B|$37.4B|
|**Revenue Growth**|High|48.9% YoY|
|**Profitability**|Very high (memory pure-play)|ROE 15.76%, Net Margin 22.8%|
|**NVIDIA Supply**|90%|Secondary supplier|
|**Political Risk**|Tariffs (Korean)|Protected (U.S. + CHIPS Act)|
|**Subsidy**|None|$6.17B grants + $7.5B loans|
|**Moat Strength**|⭐⭐⭐⭐⭐ (tech leader, 90% NVIDIA)|⭐⭐⭐⭐ (CHIPS Act, domestic)|
|**Valuation vs Growth**|**Cheap** (8x forward P/E)|Fair (12.3x forward P/E)|
|**Expected Return (CAGR)**|**+19.0%**|+12.9%|
|**Allocation**|**60%**|**40%**|

---

## Part 5: Allocation & Math

## Tariff Probability Analysis

**This is my biggest uncertainty.** I need to be transparent about it.

I used multiple sources to build tariff probabilities:

- Polymarket prediction markets
    
- Historical U.S.-Korea trade negotiations (steel, autos, semiconductors)
    
- Trump administration statements (Jan 14, Jan 17 tariff actions)
    
- Korea's response (downplaying, citing KORUS FTA)
    

**My tariff probability distribution:**

|Tariff Level|Probability|Description|
|---|---|---|
|**None (0-5%)**|10%|Korea secures exemption via KORUS FTA|
|**Symbolic (10-15%)**|60%|Negotiated outcome, Korean firms commit to U.S. fabs|
|**Moderate (25-30%)**|20%|Partial tariffs on specific products|
|**Aggressive (50%+)**|10%|Lutnick's 100% threat is realized|

**Key insight:** I'm significantly MORE bearish on tariffs than market consensus (~50% no tariff). Market is pricing 50% chance of no tariffs. I think it's only 10%. This means **I'm taking tariff risk more seriously than most investors**.

That's why I'm not going 80/20 SK hynix. I'm hedging with 40% Micron.

**Rebalancing trigger:** April 14, 2026, USTR report on semiconductor tariffs will clarify policy. That's 79 days away.

---

## Expected Returns Methodology

I used **scenario-weighted expected value** with cycle-based multiple expansion (similar to how professional analysts value cyclical companies).

**Assumptions:**

1. HBM market grows from $35B (2025) → $100B (2028) [verified via SK hynix IR, Micron IR, TrendForce]
    
2. Supply stays tight through 2027 (60% demand fulfillment in 2026) [verified]
    
3. P/E multiples expand as cycle matures (8-12x current → 25-30x at peak)
    
4. Based on 2016-2018 memory cycle precedent
    

**Returns by Scenario:**

|Tariff Scenario|Probability|SK hynix Return (CAGR)|Micron Return (CAGR)|
|---|---|---|---|
|None (0-5%)|10%|+25%|+15%|
|Symbolic (10-15%)|60%|+19%|+13%|
|Moderate (25-30%)|20%|+12%|+11%|
|Aggressive (50%+)|10%|+5%|+15%|

**Expected Value Calculation:**

**SK hynix:**

- (0.10 × 25%) + (0.60 × 19%) + (0.20 × 12%) + (0.10 × 5%)
    
- = 2.5% + 11.4% + 2.4% + 0.5%
    
- = **+19.0% CAGR**
    

**Micron:**

- (0.10 × 15%) + (0.60 × 13%) + (0.20 × 11%) + (0.10 × 15%)
    
- = 1.5% + 7.8% + 2.2% + 1.5%
    
- = **+12.9% CAGR**
    

**Blended Return (60% SK / 40% MU):**

- (0.60 × 19.0%) + (0.40 × 12.9%)
    
- = 11.4% + 5.16%
    
- = **+16.3% CAGR**
    

Over 5 years: $10,000 → $21,154 (111% total return)

---

## Kelly Criterion & Position Sizing

I used **Kelly Criterion** to determine optimal position sizing. The formula balances expected return against volatility to maximize long-term growth.

**Simplified Kelly:** f* = (Expected Return - Risk-Free Rate) / Variance

**For SK hynix:**

- Expected return: 19.0% CAGR
    
- Risk-free rate: 4% (10-year Treasury)
    
- Standard deviation: ~75% (wide range of outcomes)
    
- Kelly % = (19.0% - 4%) / (0.75²) = 15% / 0.56 = **27%**
    

**For Micron:**

- Expected return: 12.9% CAGR
    
- Risk-free rate: 4%
    
- Standard deviation: ~60% (narrower range)
    
- Kelly % = (12.9% - 4%) / (0.60²) = 8.9% / 0.36 = **25%**
    

**Total Kelly allocation:** 27% + 25% = 52% of capital

**Normalized:** 27/52 = 52%, 25/52 = 48%

**But I'm not putting 52% of my portfolio into memory.** Kelly assumes you can perfectly rebalance and handle volatility. In practice, I'm limiting this to **5% of total portfolio** (roughly 1/10th of Kelly, which is conservative).

**Within that 5%:**

- SK hynix: 52% of memory allocation → **0.52 × 5% = 2.6% of portfolio**
    
- Micron: 48% of memory allocation → **0.48 × 5% = 2.4% of portfolio**
    

**Rounded for execution: 60% SK hynix / 40% Micron**

This tilts slightly more toward SK hynix to capture the valuation gap (8x vs 12.3x forward P/E) and market leadership (62% share, 90% of NVIDIA).

---

## Part 6: Risks and Monitoring

## What Could Prove Me Wrong

I'm documenting these risks publicly so I can track which ones materialize and what I missed.

**1. AI Winter (Demand Collapse)**

If AI fails to deliver ROI for enterprises, spending could collapse. This breaks the entire thesis. Hyperscalers cut capex, inference volume drops, memory demand craters.

**My view:** 80% confidence this doesn't happen over 5 years. AI is already embedded in critical workflows. Even if hype fades, structural demand stays.

**Monitoring:** Hyperscaler capex announcements (Microsoft, Amazon, Google, Meta). If two or more cut AI spending by >25%, reassess.

---

**2. Tariffs Worse Than Expected**

If Trump's 100% tariff threat is realized and Korea doesn't secure exemption, SK hynix is severely disadvantaged. Stock could drop 20-30% on the announcement.

**My view:** 30% probability of worse-than-expected outcome (combining moderate + aggressive scenarios). This is why I'm hedging with 40% Micron.

**Monitoring:** April 14, 2026 USTR report. Clear rebalancing triggers based on outcome (see below).

---

**3. Energy Constraints Limit Deployment**

Data centers need 1 GW of power (equivalent to San Francisco's entire usage). Grid infrastructure has long permitting delays. If data centers can't get power, they can't deploy AI, even if they have chips and memory.

**My view:** Real risk, but mitigated by:

- Edge inference (distributed, not centralized power)
    
- Efficiency improvements (AI models get more efficient over time)
    
- Nuclear/renewable investment (Microsoft reopening Three Mile Island for AI)
    

**Monitoring:** Data center construction announcements. If major projects get cancelled for power reasons, reassess.

---

**4. Efficiency Gains Outpace Usage Growth**

Model compression (quantization, pruning, distillation) can reduce memory requirements by 80-95% while maintaining accuracy. If efficiency improves faster than usage grows, memory demand could plateau or decline.

**My view:** Jevons paradox applies. When something gets cheaper/more efficient, usage explodes. Cloud computing got more efficient, but data center capacity grew 10x. Same will happen with AI.

**Monitoring:** Memory content per GPU trend. If HBM per chip starts declining YoY, reassess.

---

**5. Chinese Competition Accelerates**

CXMT and YMTC are 3-5 years behind on HBM development. If they catch up faster (via IP theft, subsidies, or breakthroughs), they could flood the market with cheaper alternatives.

**My view:** Low risk through 2027. Export controls restrict their access to advanced equipment. Even if they succeed technically, U.S./Europe won't buy from them for security reasons.

**Monitoring:** CXMT/YMTC HBM announcements. If they achieve NVIDIA qualification before 2028, reassess.

---

**6. Memory Cycle Peaks Earlier (2027 vs 2028)**

Memory is cyclical. If too much capacity comes online in 2027, supply could exceed demand, prices collapse, and margins compress. SK hynix and Micron stocks would crater.

**My view:** Medium risk. This is the nature of cyclical investing. Supply is constrained through 2027, but 2028 is uncertain.

**Monitoring:** Fab capacity additions. If three or more new HBM fabs come online 2027-2028, prepare to exit or trim.

---

**7. Alternative Architectures Emerge**

If something fundamentally different displaces HBM (e.g., chiplet architectures, CXL interconnects, on-chip memory), the entire thesis breaks.

**My view:** Low risk over 5 years. HBM3/HBM4 is the current standard. Any replacement would take 5-10 years to develop and deploy at scale.

**Monitoring:** JEDEC standards body announcements. If new memory standard emerges with major industry backing, reassess.

---

## Rebalancing Plan

**Critical Milestone: April 14, 2026 (79 days away)**

The USTR report on semiconductor tariffs will clarify policy. This is my main rebalancing trigger.

**If USTR recommends <5% tariff** (Korea exemption):  
→ **Shift to 65% SK hynix / 35% Micron**

- Tariff risk eliminated, valuation advantage dominates
    
- Expected return improves to +17.5% CAGR
    

**If USTR recommends 5-15% symbolic tariff** (matches my base case):  
→ **Keep 60% SK hynix / 40% Micron**

- No action needed, allocation matches outcome
    

**If USTR recommends 15-25% moderate tariff:**  
→ **Shift to 45% SK hynix / 55% Micron**

- Higher tariff drag than modeled, Micron protection more valuable
    

**If USTR recommends >25% aggressive tariff:**  
→ **Shift to 30% SK hynix / 70% Micron**

- SK fundamentally disadvantaged, Micron quasi-monopoly in U.S.
    

**Quarterly Monitoring:**

- HBM supply/demand balance (watch for oversupply signals)
    
- Samsung qualification progress (competitive threat)
    
- Chinese HBM development timeline
    
- AI capex trends (hyperscaler spending sustainability)
    

**Exit Scenarios:**

- AI inference demand declines 6+ months consecutively
    
- HBM oversupply emerges (2027-2028)
    
- New memory technology disrupts HBM
    

---

## Conviction Level

**High conviction on the thesis** (memory wins across AI paradigms)  
**Medium conviction on timing** (5-year horizon reduces timing risk)  
**Medium conviction on tariff outcome** (biggest uncertainty)

**Overall: 75% conviction**

I'm sizing this at **5% of portfolio** (conservative vs Kelly's 52%) because:

1. Tariff uncertainty is real
    
2. Memory is cyclical, timing matters
    
3. I want dry powder for other opportunities
    
4. 5% is meaningful but not career-ending if I'm wrong
    

**What would increase conviction to 90%+:**

- April 14 USTR report shows <10% tariffs (clarity on geopolitical risk)
    
- Q2-Q3 2026 earnings show accelerating HBM revenue (validates demand)
    
- SK hynix or Micron announces major U.S. fab expansion (de-risks tariffs)
    

---

## Closing: Why I'm Sharing This

This thesis is imperfect. I'll likely look back in a year and see things I got wrong or should have dug deeper on. But I'm publishing it anyway for three reasons:

**1. Accountability.** By timestamping this on GitHub, I can't backfill my analysis or cherry-pick winners. If I'm wrong, I'll document what I missed.

**2. Learning in public.** The best way to improve is to put your ideas out there and get feedback. If you see flaws in this analysis, I genuinely want to hear them.

**3. Building a process.** This is my attempt to build a disciplined, repeatable framework for researching investments: identify macro themes, map technical details, find mispriced opportunities, size positions mathematically, and monitor with clear triggers.

**The game here isn't to be right about every detail.** It's to:

- Ask better questions than the market
    
- Build a thesis that's resilient across scenarios
    
- Size positions appropriately for uncertainty
    
- Have a plan for what changes my mind
    

If this thesis works, great. If it doesn't, I'll learn why and get better.

Either way, I'm committed to updating this publicly as events unfold.

---

**Next review:** February 28, 2026 (monthly performance check)  
**Critical milestone:** April 14, 2026 (USTR semiconductor tariff report)

---

## Citations and Sources

## Primary Research

- Steve Eisman, "Gary Marcus on the Massive Problems Facing AI & LLM Scaling," _The Real Eisman Playbook_, Episode 42, January 19, 2026. [YouTube](https://www.youtube.com/watch?v=aI7XknJJC5Q)
    
- Gary Marcus, "Deep Learning Is Hitting a Wall," 2022
    
- Yann LeCun, AMI Labs founding and world models strategy, January 2026
    
- MIT Technology Review, "Yann LeCun's new venture is a contrarian bet against large language models," January 22, 2026
    

## Financial Data

- SK hynix Investor Relations, "2026 Market Outlook: Focus on the HBM-led Memory Supercycle," January 4, 2026
    
- Micron Technology Q2 FY2026 earnings report
    
- Stock prices via MarketScreener, Stock Analysis, NASDAQ (January 24, 2026)
    
- Forward P/E ratios verified via multiple analyst consensus sources
    

## Market Research

- Mordor Intelligence, "AI Chipsets Market Size & Analysis," January 11, 2026
    
- Fortune Business Insights, "AI Inference Market Report," January 21, 2026
    
- Precedence Research, "High Bandwidth Memory Market," October 13, 2025
    
- TrendForce, HBM supply/demand analysis, 2025-2026
    
- Bank of America, AI memory TAM projections, January 2026
    

## Tariff and Policy

- Reuters, "Trump imposes 25% tariff on imports of some AI chips," January 14, 2026
    
- U.S. Commerce Secretary Howard Lutnick, Micron groundbreaking ceremony remarks, January 17, 2026
    
- CHIPS and Science Act awards to Micron ($6.17B grants, $7.5B loans), December 2024
    
- South Korea trade policy responses, January 2026
    

## Technical Research

- OpenReview, "Scaling LLM Test-Time Compute Optimally," 2024-2025
    
- Nature, "How good old-fashioned AI could spark the field's next revolution," November 24, 2025
    
- KU Leuven, "Efficient Hardware Architectures for Neuro-Symbolic AI," ongoing research
    
- NVIDIA, "What Is a World Model?" October 27, 2025
    

---

**Last updated:** January 24, 2026  
**Version:** 1.0  
**License:** CC BY 4.0 (Attribution required for reuse)